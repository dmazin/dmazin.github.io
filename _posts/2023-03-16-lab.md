---
layout: post
title: "Lab Notes: 16 Mar 2023"
byline: By <a href="https://www.cyberdemon.org/">Dmitry Mazin</a>.
date: 2023-03-16
tags: labs
---
This week, instead of posting daily lab notes I’ve been working silently. I’ve still been taking notes, but have taken less care to make them presentable. My idea has been that instead of posting daily lab notes, which are unlikely to be interesting, I ought to just finish up whatever thing I’m investigating, and then put together a polished narrative. It’s still not clear to me what the right approach is. Today, I think I will try a hybrid: the lab notes will cover only what I worked on today (and so, if I find motivation, I should be able to publish a new lab note daily), but I will polish the narrative a bit.

When I’m done investigating this, I plan on writing a polished article, with a direct narrative, that I hope will be more broadly interesting than my lab notes.

Anyway, to recap, for days now I’ve been investigating why sysbench’s reported random write throughput is lower than the disk throughput reported by the Linux kernel (reported via `sar` and `iostat`, that is).

Earlier this week, I looked into exactly how sysbench calculates its throughput to understand how it works, and to see if there is maybe some bug. But, no, I do not think there is any bug at all. sysbench properly calculates exactly how much it writes, and then divides that by the runtime to calculate the throughput. (I will explain this in more detail in the final article. Here, I just want to recap what I’ve been doing).

Then, I had a hunch: what if disks have a minimum “write unit”, so that if you request a 16 KB write, they will still write 128 KB (for example), because they cannot write less than 128 KB at a time. To test this, I started playing with sysbench’s chunk size parameter, which controls how many bytes at a time sysbench writes. My hypothesis was that, if the sysbench chunk size is smaller than the disk “write unit”, then if I double, or halve, the chunk size, I should not see significant change in the disk throughput. Indeed, this is what I found!

At this point, I realized that things would make a lot more sense to me if I understood the fundamentals better. To understand the fundamentals, I picked up an excellent book by Brendan Gregg, System Performance (2nd edition). Gregg takes an excellent approach to teaching performance engineering: first, he explains the fundamentals, then he explains how to observe whatever it is you want to diagnose or tune (e.g. disk performance), and then he explains how to tune it.

Reading Gregg, I learned that there are two systems I need to understand in order to understand what is going on during the sysbench benchmark. Obviously, there is the disk system. I need to understand what is happening on the disk, because it might be buffering writes, etc. Less obviously, I also need to learn about the file system. The file system also does a bunch of stuff, like buffering, before the write request hits the disk.

One thing I learned right away is that there is a distinction between logical writes and physical writes. Logical writes are, for example, `pwrite` calls, which is the syscall sysbench uses to ask the file system to write to a random file location. Physical writes refer to the disk actually bring written to. The fact that there is a distinction is important: not to give the whole investigation away, but after reading Gregg, it is not surprising to me that sysbench and the kernel are giving me different stats: sysbench is counting logical bytes written, while the kernel is counting the actual bytes written to disk.

At this point, I think I have a good idea for what I want to do next.
* Identify the sub-components/features of the ext4 file system, as well as my SSD, which may account for the discrepancy between sysbench’s reported throughput and disk throughput.
* See if I can tune various parameters to get the two throughputs to match. If I can get them close, it shows that I have properly understood what happens when you try to do a random disk write.

I have almost finished reading the Gregg chapter on file systems. I need to read the bit about tuning, because it may help continue to explain the features and sub-systems of my file system. After that, I will read the chapter on disks.

Ah, actually, the file system chapter has some advice about benchmarking, which I should read before reading about tuning.

Funny enough, Gregg advises that your benchmarking tool may not agree with `iostat`:

> When using these tools, it is a good idea to leave iostat(1) continually running to confirm that the workload reaches the disks as expected, which may mean not at all. For example, when testing a working set size that should easily fit in the file system cache, the expectation with a read workload is 100% cache hits, so iostat(1) should not show substantial disk I/O.

Given what Gregg teaches about the file system, this is not at all surprising. I suppose this would make no sense to you, though, because I have not actually explained any of the features of a file system that may explain the discrepancy. I will leave that for the final article.

When talking about `dd`, Gregg mentions that a sequential write test performed using `dd` will not really tell you disk performance so much as file system performance. One reason is that, thanks to write-back caching, the writes may not actually be flushed to disk. Gregg references the `vm.dirty_* tunable` parameters, which is covered in yet another chapter: memory. I sincerely hope that I will not need to read the memory chapter as part of this entire investigation, though it kind of makes sense. Memory is highly relevant to file systems.

Next, Gregg mentions which benchmarking tools he uses to benchmark file systems. He does mention sysbench, though his recommendation is to use `fio` as it more accurately simulates real-world access patterns, and, hugely importantly, reports latency percentiles. Perhaps at the end of all this, I will also play with `fio`. Also, I learned something interesting: the thing I’ve been doing, where I run a benchmarking tool while also monitoring using observability tools, is called “active benchmarking”. Gregg says: “You can confirm that the benchmark tests what it says it tests, and that you understand what that is. […] As a bonus, this can be a good time to develop your skills with performance observability tools.” Indeed.

Ooh, next Gregg teaches about a very useful thing you can do. It’s possible for two consecutive benchmarks to perform differently, because the first ran on a cold cache, and warmed up the cache during the benchmark, while the second benchmark ran against a warm cache. `echo 3 > /proc/sys/vm/drop_caches` will free the filesystem cache entirely.

In terms of tuning, I’m particularly interested in tuning the file system. For example, perhaps I can tune the record size? One thing you can tune is that you can turn off some metadata updates via your `mount` options. For example, you can set `noatime` to avoid changing the access times. This might be useful when exploring “when I write 4 KB to disk, how come more than 4 KB gets written?” Anyway, back to record size. When I list filesystem parameters using `tune2fs -l <device>`, I see that block size is set to 4096.

I am interested in changing the block size, and Gregg does not explain. So I went searching online, and a [SO answer](https://unix.stackexchange.com/a/237713/467945) mentions that I can get various block device parameters using `blockdev` and I can change some of them when mounting the filesystem.

Reading the `blockdev` manpage, I see some interesting parameters it can read.

```
       --getiomin
           Get minimum I/O size.

       --getioopt
           Get optimal I/O size.

       --getpbsz
           Get physical block (sector) size.

       --getss
           Print logical sector size in bytes - usually 512.

       --getbsz
           Print the blocksize in bytes. This size does not describe device topology. It’s the size used internally by the kernel and it may be modified (for example) by filesystem
           driver on mount.
```

There is a lot going on here. There is I/O size, physical block size, logical sector size, and block size. What are the values?

```
$ sudo blockdev --getiomin --getioopt --getpbsz --getss --getbsz /dev/sda5

4096
0
4096
512
4096
```

I have no idea what I/O size is, but one of the 4096’s is surely the ext4 block size, and one of them must be the disk sector size. I do not know what the third 4096 is.

I think I am getting a little bit into the weeds. Let’s earmark for later that I will want to play with creating a new filesystem later with a different block size, which I can do with `mkfs -t ext4 -b 4096 <device>`.

This wraps up the chapter on file systems. Though, I did not read much about file system observability tools. I may come back to that section after reading about disk observability tools. I’d like to be able to tell which tools tell you about the *disk* and which tell you about the *file system*.

Now, I am going to read the chapter on disks.

One thing I see right away: disks have internal queues. In computing, queues are everywhere (this is one concept I want to write about sometime).
![](/assets/disk-queue.png)

Another interesting thing: a disk may have an internal cache, used not only for reads, but also writes (in which case we’d call it a write buffer). It is possible for a disk to tell the kernel the write succeeded once it has reached the write buffer. This is fine, apparently, because such a cache would have batteries meaning that the write can be finished later.

Gregg goes on to discuss the intricacies of disk timing in detail. For example, the difference between “block I/O wait time” and “disk wait time”. I am interested in timing things, but not right now, so I am mostly glossing over this.

Ah, OK, here we go, Gregg is explaining what I/O sizes are. The I/O size a characteristic of your workload. That is, it’s not something you set directly. For example, you could plot the distribution of I/O sizes for your workload, and determine if it’s most efficient or not. I/O size is affected by things like the disk sector size, so, for example, if you do a bunch of 1-byte writes, the average I/O size may be 512 bytes (because the disk sector size, in this instance, is 512 bytes). There is an optimal I/O size for your system, which may be documented, but can be found by benchmarking. So, for example, I could tune the sysbench parameters (e.g. chunk size) to get maximum performance out of it. I could use this information to inform decisions, e.g. database index page size.

Welp, my brain is saturated. I don’t think I can read any more.